{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E9UtJfIIGbCq"
      },
      "source": [
        "# PA1: K-Nearest Neighbors\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/nn-k1.png\">\n",
        "</center>\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this assignment, you will be creating your first Machine Learning model from scratch: K-Nearest Neighbors.\n",
        "\n",
        "This algorithm is one of the simpler ones you will come across, but the ideas can be applied to large-scale sophisticated systems: Semantic Search and Recommendation Systems for starters.\n",
        "\n",
        "For this assignment, you will be creating your own KNN-classifier from scratch using `numpy`. You can then use this to classify images of _handwritten digits_ from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This is the \"Hello World\" of Machine Learning.\n",
        "\n",
        "After this notebook you should be able to:\n",
        "\n",
        "- Utilize `numpy` to implement a simple KNN classifier from scratch\n",
        "\n",
        "- Understand how to setup a good Cross Validation strategy\n",
        "\n",
        "- Be able to setup simple classification tasks\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- Follow along with the notebook, filling out the necessary code where instructed.\n",
        "\n",
        "- <span style=\"color: red;\">Read the Submission Instructions and Plagiarism Policy in the attached PDF.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
        "\n",
        "- <span style=\"color: red;\">Do not remove any pre-written code.</span> We will be using the `print` statements to grade your assignment.\n",
        "\n",
        "- <span style=\"color: red;\">You must attempt all parts.</span> Do not assume that because something is for 0 marks, you can leave it - it will definitely be used in later parts.\n",
        "\n",
        "- <span style=\"color: red;\">Do not use unauthorized libraries.</span> You are not allowed to use `sklearn` in Part 1. Failure to follow these instructions will result in a serious penalty.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qMxzaKw93VoI"
      },
      "source": [
        "## Part 1: KNNs from Scratch [75 marks]\n",
        "\n",
        "Again, you are <span style=\"color: red;\">not allowed</span> to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own k-NN classifier from scratch.\n",
        "\n",
        "You can use `numpy`, `pandas`, `seaborn`, `matplotlib`, `PIL`, and the standard Python libraries for this part. Contact the TAs if you want to use any other libraries.\n",
        "\n",
        "### Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TjOjOx_YwyII"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import PIL"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rxZ0oTZ1Lwf4"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/mnist.png\">\n",
        "</center>\n",
        "\n",
        "The MNIST dataset consists of 70,000 labeled images of handwritten digits, each of size 28 pixels by 28 pixels.\n",
        "\n",
        "The dataset given to you is in a CSV file, `mnist.csv`. The CSV file has ~70,000 rows and 785 columns. You can download it using [this link](https://drive.google.com/file/d/16STvH3jEk-JF1BGAguhnA9y0sT_c_MC-/view?usp=sharing).\n",
        "\n",
        "- Each row represents one image of a handwritten digit. Note that the header row contains the column names.\n",
        "\n",
        "- The first column gives the label (a number from 0 to 9). The next 784 columns give the value of each pixel. There are 784 pixels in each image corresponding to an image size of 28 by 28.\n",
        "\n",
        "For faster prototyping, you can sample 20% of the entire dataset.\n",
        "\n",
        "You can use the `pandas` library to load the CSV file but the final dataset should be stored in a `numpy` array of shape (14000, 785).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUjljzTQ1rN_",
        "outputId": "7de4b3d0-b0eb-4f5a-ba07-c2e58313a1a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape : (70000, 785)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>...</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
              "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
              "\n",
              "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
              "0      0      0      0      0      0      0      0      0  \n",
              "1      0      0      0      0      0      0      0      0  \n",
              "2      0      0      0      0      0      0      0      0  \n",
              "3      0      0      0      0      0      0      0      0  \n",
              "4      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## (3 marks)\n",
        "\n",
        "# TODO: Load the dataset\n",
        "df = pd.read_csv(\"mnist.csv\", header=1)\n",
        "# df.drop(df.index[0])\n",
        "arr = df.to_numpy()\n",
        "# arr = arr.astype(int)\n",
        "\n",
        "# print(f\"shape : {arr.shape}\")\n",
        "\n",
        "\n",
        "# TODO: Print the shapes\n",
        "print(f\"Shape : {arr.shape}\")\n",
        "\n",
        "\n",
        "# TODO: Display the first 5 rows of the dataset\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2FiUkM6yw6SX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "# (2 marks)\n",
        "# TODO: Sample 20% of the dataset (Make sure this is a random sample!)\n",
        "# Please note that this is not the train-test split. This is just a sample of the dataset. We are doing this to reduce the computation time.\n",
        "sampled = df.sample(n=14000, random_state=22)\n",
        "arr = np.array(sampled)\n",
        "# new = np.delete(arr, 0, axis=1)\n",
        "print(arr[0][0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8d2EtnB9PB8x"
      },
      "source": [
        "### Displaying Images\n",
        "\n",
        "Now that you've loaded the dataset, let's display some images.\n",
        "\n",
        "You can reshape these 784 values for each image, into a `28x28` array, then use either `matplotlib` or `PIL` to display the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lZ7AbgWjPA4h"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a display_image function (3 marks)\n",
        "\n",
        "iterations = 1\n",
        "\n",
        "\n",
        "def display_image(arr):\n",
        "    \"\"\"\n",
        "    Takes a 1D numpy array, reshapes to a 28x28 array and displays the image\n",
        "    \"\"\"\n",
        "    # print(arr)\n",
        "    # print(len(arr))\n",
        "    new = np.delete(arr, 0)\n",
        "    # print(len(new))\n",
        "    new = new.reshape(28, 28)\n",
        "    # print(new.shape)\n",
        "    # print(new)\n",
        "    # print(f\"this number is : {arr[0]}\")\n",
        "    plt.subplot(1, 5, iterations)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "5\n",
            "1\n",
            "1\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Randomly pick and display 5 images from the dataset\n",
        "\n",
        "## Code here\n",
        "\n",
        "# pick a random row\n",
        "for _ in range(5):\n",
        "    random_index = np.random.randint(0, len(arr))\n",
        "    random_img = arr[random_index, :]\n",
        "    print(arr[random_index, 0])\n",
        "    display_image(random_img)\n",
        "    iterations += 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ND2Dc_EOQzCg"
      },
      "source": [
        "### Train-Test split\n",
        "\n",
        "With the data loaded, you should set up a proper Cross Validation scheme for your modeling experiments, before you actually start building your model.\n",
        "\n",
        "Divide the dataset into training and test sets (around an 85-15 split). More precisely, take the first 11900 images for the training set and the last 2100 for the test set.\n",
        "\n",
        "Both the resulting splits/sets should be stored in `numpy` arrays of shape `(num_split_images, 785)`. Depending on your approach, you can also separate the labels into a different array (or two arrays)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1iBwQIMsQ1wd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n",
            "Shape data_train : (11900, 785)\n",
            "Shape data_test : (2100, 785)\n"
          ]
        }
      ],
      "source": [
        "## (3 marks)\n",
        "# TODO: Create a train-test split (2 marks)\n",
        "labels = arr[:, 0]\n",
        "# print((labels.shape))\n",
        "\n",
        "data_train = arr[:11900, :]\n",
        "data_test = arr[11900:, :]\n",
        "print(data_train[0][0])\n",
        "\n",
        "\n",
        "# TODO:Print the shapes of both arrays (1 mark)\n",
        "print(f\"Shape data_train : {data_train.shape}\\nShape data_test : {data_test.shape}\")\n",
        "# print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ro0MSTLjFiuL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_train: (11900, 784)\n",
            "input_test: (2100, 784)\n",
            "label_train: (11900,)\n",
            "label_test: (2100,)\n"
          ]
        }
      ],
      "source": [
        "## (3 marks)\n",
        "# TODO: Split the train and test into inputs and labels (2 marks)\n",
        "input_train = data_train[:, 1:]\n",
        "input_test = data_test[:, 1:]\n",
        "\n",
        "label_train = labels[:11900]\n",
        "label_test = labels[11900:]\n",
        "\n",
        "# TODO: Print the shapes of the 4 arrays (1 mark)\n",
        "print(\n",
        "    f\"input_train: {input_train.shape}\\ninput_test: {input_test.shape}\\nlabel_train: {label_train.shape}\\nlabel_test: {label_test.shape}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HUX1FaJfpVb0"
      },
      "source": [
        "### Implementing k-NN Classifier\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "Now you can create your own k-NN classifier. You can use the following steps as a guide:\n",
        "\n",
        "1. For a test data point, find its distance from all training instances.\n",
        "\n",
        "2. Sort the calculated distances in ascending order based on distance values.\n",
        "\n",
        "3. Choose k training samples with minimum distances from the test data point.\n",
        "\n",
        "4. Return the _most frequent_ class of these samples.\n",
        "\n",
        "**Note:** Your function should work with _Euclidean_ distance as well as _Manhattan_ distance. Pass the distance metric as a parameter in the k-NN classifier function. Your function should also let one specify the value of `k`.\n",
        "\n",
        "For values of `k` where a tie occurs, you need to break the tie by backing off to the `k-1` value. In case there is still a tie, you will continue decreasing `k` until there is a clear winner.\n",
        "\n",
        "#### Distance functions\n",
        "\n",
        "First, implement separate functions for the Euclidean and Manhattan distances. Formulas for both are given below.\n",
        "\n",
        "$$\n",
        "d_{\\text{Euclidean}}(\\vec{p},\\vec{q}) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + (p_3 - q_3)^2 + ... + (p_n - q_n)^2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "d_{\\text{Manhattan}}(\\vec{p},\\vec{q}) = |(p_1 - q_1)| + |(p_2 - q_2)| + |(p_3 - q_3)| + ... + |(p_n - q_n)|\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_d287bYwx-B_"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(vector1, vector2):\n",
        "    # TODO:Implement the euclidean distance function (2 marks)\n",
        "\n",
        "    t = vector1 - vector2\n",
        "    ans = np.sqrt(np.dot(t.T, t))\n",
        "    return ans\n",
        "\n",
        "\n",
        "def manhattan_distance(vector1, vector2):\n",
        "    # TODO:Implement Euclidean and Manhattan distance functions (2 marks)\n",
        "    ans = np.sum(np.abs(vector1 - vector2))\n",
        "    return ans\n",
        "\n",
        "\n",
        "#########################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLQPLGZYLus"
      },
      "source": [
        "#### k-NN Classifier methods\n",
        "\n",
        "Complete the following method functions:\n",
        "\n",
        "1. `fit`\n",
        "\n",
        "2. `get_neighbours`\n",
        "\n",
        "3. `predict`\n",
        "\n",
        "You can make as many helper functions as you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v-vNkZ1dOtXz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class kNearestNeighbours:\n",
        "\tdef __init__(self, num_neibrs):\n",
        "\t\t### DO NOT EDIT !! ###\n",
        "\t\t\"\"\"\n",
        "\t\tn_neighbours: value of k\n",
        "\t\tX: array of training data points\n",
        "\t\ty: array of gold labels for training points\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tself.n_neighbors = num_neibrs\n",
        "\t\tself.X = None\n",
        "\t\tself.y = None\n",
        "\n",
        "\t#######################\n",
        "\t#data and its labels passed\n",
        "\tdef fit(self, X_train, y_train):\n",
        "\t\t\"\"\"\n",
        "\t\tFit the training data to the model\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\t# TODO: Set both attributes (1 mark)\n",
        "\n",
        "\t\t#x_train has 784 col, y_train are the labels\n",
        "\t\tself.X = X_train\n",
        "\t\tself.y = y_train\n",
        "\n",
        "\n",
        "\tdef get_neighbors(self, x, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn the k nearest neighbours of the input data point x.\n",
        "\t\tHint: you can even just return the indices of the data points\n",
        "\n",
        "\t\t\"\"\"\n",
        "\t\t# TODO: Complete the get_neighbors function (4 marks)\n",
        "\t\tans = []\n",
        "\t\tfor i in range(0, len(self.X)):\n",
        "\t\t\ttemp = self.X[i, :]\n",
        "\n",
        "\t\t\t# #if the same point used\n",
        "\t\t\t# if np.array_equal(x, temp):\n",
        "\t\t\t# \tcontinue\n",
        "\n",
        "\t\t\td = distanceFunction(x, temp)\n",
        "\t\t\tans.append((d, i))\n",
        "\t\t\t\n",
        "\t\tans.sort()\n",
        "\n",
        "\t\t#choosing the k nearest points\n",
        "\t\tslice_ans = ans[0:self.n_neighbors]\n",
        "\t\tindices = []\n",
        "\t\tfor i in range(0, self.n_neighbors):\n",
        "\t\t\ttemp = slice_ans[i]\n",
        "\t\t\ttemp = temp[1]\n",
        "\t\t\t#the k nearest indices\n",
        "\t\t\tindices.append(temp)\n",
        "\n",
        "\t\treturn indices\n",
        "\n",
        "\tdef predict(self, X_test, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns an array of predicted labels for all points in the X_test array\n",
        "\n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tX_test : array\n",
        "\t\t\t\t\t\tThe test data\n",
        "\n",
        "\t\tdistanceFunction : function\n",
        "\t\t\t\t\t\tThe distance function to be used\n",
        "\t\t\"\"\"\n",
        "\t\t# TODO: Complete the predict function (5 marks)\n",
        "\t\tans_arr = []\n",
        "\t\t\n",
        "\t\tfor i in range(0, len(X_test)):\n",
        "\t\t\t#getting k narest neighbours for every point in the test data\n",
        "\t\t\tindices = self.get_neighbors(X_test[i], distanceFunction)\n",
        "\t\t\tflag = True\n",
        "\n",
        "\t\t\tk = self.n_neighbors\n",
        "\t\t\t#run the loop as long as there is no tie\n",
        "\t\t\twhile flag:\n",
        "\n",
        "\t\t\t\t#stores the closest n neighbours\n",
        "\t\t\t\tmax_count = [-1, -1]\n",
        "\n",
        "\t\t\t\t# the closest k labels\n",
        "\t\t\t\ttemp_list = []\n",
        "\t\t\t\t# count = [] \n",
        "\t\t\t\tfor j in range(0, k):\n",
        "\t\t\t\t\ttemp_list.append(self.y[indices[j]])\n",
        "\n",
        "\t\t\t\t# sorts according to the digit that appears tthe most\n",
        "\t\t\t\tfor j in range(0, 10):\n",
        "\n",
        "\t\t\t\t\t#temp stores the no of times j appears\n",
        "\t\t\t\t\ttemp = temp_list.count(j)\n",
        "\t\t\t\t\tif temp == max_count[0]:\n",
        "\t\t\t\t\t\tflag = True\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\telif temp > max_count[0]:\n",
        "\t\t\t\t\t\tflag = False\n",
        "\n",
        "\t\t\t\t\t\t#the no of times j appears\n",
        "\t\t\t\t\t\tmax_count[0] = temp\n",
        "\t\t\t\t\t\t#what the digit is\n",
        "\t\t\t\t\t\tmax_count[1] = j\n",
        "\n",
        "\t\t\t\tif flag:\n",
        "\t\t\t\t\tk -= 1\n",
        "\t\t\t\t\t\n",
        "\t\t\t\telif not flag:\n",
        "\t\t\t\t\tans_arr.append(max_count[1])\n",
        "\n",
        "\t\t#the k nearest neighbur for every index\n",
        "\t\treturn ans_arr\t\t\t\n",
        "\n",
        "\t\t\n",
        "\t\t# return ???"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optional Challenge:\n",
        "\n",
        "Using for loops can be really slow so in order to improve performance we can leverage the power of vectorization. Try vectorizing the process of finding the distance from a query point to each point in the dataset.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxV02qUIwutD"
      },
      "source": [
        "#### Evaluation\n",
        "\n",
        "Now that you've created a model and \"trained\" it, you can move on to the Evaluation phase.\n",
        "\n",
        "- Implement an `evaluate` function that computes the Confusion Matrix, Accuracy, and Macro-Average F1 score of your classifier.\n",
        "- The function should take as input the predicted labels and the true labels. This will be built in steps: its easier to create a Confusion Matrix, then calculate things like the Precision, Recall and F1 from it.\n",
        "\n",
        "- We will also implement a function that displays our confusion matrix as a heatmap annotated with the data values.\n",
        "- The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n",
        "- You can have a look at some examples of heatmaps [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html). (You don't have to use the seaborn libray, but it has some pretty colour palettes to choose from.)\n",
        "\n",
        "We recommend that you do not use hard coding in this function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8OGpzp-sw6SZ"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predicted_labels, true_labels):\n",
        "    '''\n",
        "    Returns the accuracy of the predictions against the true labels\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    predicted_labels : array\n",
        "\n",
        "    true_labels : array\n",
        "    \"\"\"\n",
        "    '''\n",
        "    total = len(predicted_labels)\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, total):\n",
        "        if predicted_labels[i] == true_labels[i]:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = (correct / total) * 100\n",
        "    return accuracy\n",
        "    # TODO: Implement a function to calculate accuracy (2 marks)\n",
        "    \n",
        "\n",
        "# classifier = kNearestNeighbours(5)\n",
        "# classifier.fit(input_train, label_train)\n",
        "# test = input_test[:100, :]\n",
        "# ans = classifier.predict(test, euclidean_distance)\n",
        "# test_label = label_test[:300]\n",
        "# a = calculate_accuracy(ans,test_label)\n",
        "# print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UCQtcL-1w6SZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# print(a)\n",
        "def make_confusion_matrix(predicted_labels, true_labels):\n",
        "    '''\n",
        "    Computes the confusion matrix as a 2D array\n",
        "    '''\n",
        "\n",
        "    matrix = np.zeros((10,10))\n",
        "    for i in range(0, len(predicted_labels)):\n",
        "        prediced = int(predicted_labels[i])\n",
        "        true = int(true_labels[i])\n",
        "        matrix[prediced][true] += 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "    \n",
        "    # TODO: Implement a function to compute the confusion matrix (2 marks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HyKsRhQTw6SZ"
      },
      "outputs": [],
      "source": [
        "def make_heat_map(data, title):\n",
        "    \"\"\"\n",
        "    Creates a heatmap from the 2D matrix input\n",
        "    \"\"\"\n",
        "    sns.heatmap(data=data, annot=True)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    # TODO: Implement a funtion to display a heatmap (2 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hPvVPmnpw6SZ"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(confusion_matrix,class_label):\n",
        "    '''\n",
        "    Calculates the precision from a provided confusion matrix\n",
        "    '''\n",
        "    p_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        p_sum += confusion_matrix[class_label][i]\n",
        "    \n",
        "    ans = confusion_matrix[class_label][class_label] / p_sum \n",
        "    \n",
        "    return ans\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7GuPdo1jw6Sa"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(confusion_matrix,class_label):\n",
        "    '''\n",
        "    Calculates the recall from a provided confusion matrix\n",
        "    '''\n",
        "    # TODO: Implement a function to compute the recall (2 marks)\n",
        "    \n",
        "    r_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        r_sum += confusion_matrix[i][class_label]\n",
        "\n",
        "    ans = confusion_matrix[class_label][class_label] / r_sum\n",
        "    \n",
        "    return ans\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "olatBHW1w6Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "    '''\n",
        "    Calculates the F1 score from a provided precision and recall\n",
        "    '''\n",
        "    numerator = 2 * precision * recall\n",
        "    denominator = precision + recall\n",
        "    # TODO: Implement a function to compute the F1 score (2 marks)\n",
        "    \n",
        "    return (numerator / denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ydkPM5Fgw6Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def macro_average_f1(confusion_matrix):\n",
        "    '''\n",
        "    Calculates the macro-average F1 score from a provided confusion matrix, over all classes\n",
        "    '''\n",
        "    # TODO: Implement a function to compute the Macro-average F1 (2 marks)\n",
        "    f1_sum = 0\n",
        "    for i in range(0, 10):\n",
        "        precision = calculate_precision(confusion_matrix, i)\n",
        "        recall = calculate_recall(confusion_matrix, i)\n",
        "        f1 = calculate_f1_score(precision, recall)\n",
        "        f1_sum+= f1\n",
        "    \n",
        "\n",
        "    return (f1_sum / 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CpnqQnIoN7iu"
      },
      "outputs": [],
      "source": [
        "def evaluation(predicted_labels, true_labels):\n",
        "    \"\"\"\n",
        "    Computes the Confusion Matrix, Accuracy and Macro-average F1 score from the predictions and true labels\n",
        "    \"\"\"\n",
        "    ## Now put it all together using the functions you've already written above.\n",
        "    confusion_matrix = make_confusion_matrix(predicted_labels, true_labels)\n",
        "    accuracy = calculate_accuracy(predicted_labels, true_labels)\n",
        "    macroF1 = macro_average_f1(confusion_matrix)\n",
        "    # TODO: Complete the evaluation function (2 marks)\n",
        "\n",
        "    return confusion_matrix, accuracy, macroF1\n",
        "\n",
        "\n",
        "#########################"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMuyMEWeADn"
      },
      "source": [
        "#### `k`-fold Cross Validation\n",
        "\n",
        "<center>\n",
        "    <img src=\"./assets/kfoldcv.png\">\n",
        "</center>\n",
        "\n",
        "Now with the basics done, you can move on to the next step: `k`-fold Cross Validation. This is a more robust way of evaluating your model since it uses all the data for training and testing (effectively giving you `k` chances to verify the generalizability of your model).\n",
        "\n",
        "Now, implement a function that performs `k`-fold cross-validation on the training data for a specified value of `k`.\n",
        "\n",
        "In Cross Validation, you divide the dataset into `k` parts. `k-1` parts will be used for training and `1` part will be used for validation. You will repeat this process `k` times, each time using a different part for validation. You will then average the results of each fold to get the final result. Take a look at the image above for a better understanding.\n",
        "\n",
        "The function should return **predictions** for the **entire training data** (size of list/array should be equal to the size of the dataset). This is the result of appending the predicted labels for each validation-train split into a single list/array. Make sure the order of the predicted labels matches the order of the training dataset, so that they may directly be passed to your `evaluate` function together with the actual labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iLDVaOIheMFr"
      },
      "outputs": [],
      "source": [
        "def k_fold_split(k, cv_no, data):\n",
        "    \"\"\"\n",
        "    Returns the training and validation sets for a given value of k\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    k : int\n",
        "        The value of k\n",
        "    cv_no : int\n",
        "        The current fold number\n",
        "    data : array\n",
        "        The dataset\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement a function that creates the train and test splits based off the value of k (5 marks)\n",
        "    # Code here\n",
        "    train = []\n",
        "    split_data = np.array_split(data, k)\n",
        "    \n",
        "    for i in range(0, k):\n",
        "        if i != cv_no:\n",
        "            train.append(split_data[i])\n",
        "\n",
        "    \n",
        "    training_set = np.concatenate(train)\n",
        "    validation_set = split_data[cv_no]\n",
        "\n",
        "    return training_set, validation_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "twm1MAloljI0"
      },
      "outputs": [],
      "source": [
        "def k_fold_cross_validation(num_folds, k, dataset, distanceFunction):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the predictions for all the data points in the dataset using k-fold cross validation\n",
        "\n",
        "\t\tnum_folds: int\n",
        "\t\t\tNumber of folds\n",
        "\t\tk: int\n",
        "\t\t\tNumber of neighbours to consider (hyperparameter)\n",
        "\t\tdataset: array\n",
        "\t\t\tThe dataset to be used (note that this should be the training set which has 11900 samples)\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t# TODO: Implement function to perform k-fold cross-validation, using the above function (10 marks)\n",
        "\t\t## Code here\n",
        "\n",
        "\t\tprediction_list = []\n",
        "\t\ttrue_list = []\n",
        "\n",
        "\t\tfor i in range(0, num_folds):\n",
        "\t\t\t\ttraining, test = k_fold_split(num_folds, i, dataset)\n",
        "\t\t\t\tclassifier = kNearestNeighbours(k)\n",
        "\t\t\t\tx_train = training[:, 1:]\n",
        "\t\t\t\ty_train = training[:, 0]\n",
        "\t\t\t\tx_test = test[:, 1:]\n",
        "\t\t\t\ty_test = test[:, 0]\n",
        "\t\t\t\tclassifier.fit(x_train, y_train)\n",
        "\t\t\t\tpredict_label = classifier.predict(x_test, distanceFunction)\n",
        "\t\t\t\ttemp = np.array(predict_label)\n",
        "\t\t\t\tprediction_list.append(temp)\n",
        "\t\t\t\ttrue_list.append(y_test)\n",
        "\t\t\n",
        "\t\tpred = prediction_list\n",
        "\t\tgold = true_list\n",
        "\t\t\n",
        "\t\treturn pred, gold\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mWUgqOBHlsyK"
      },
      "source": [
        "Now run your cross-validation function on the training data using `5-fold cross validation` for the values of `k = [1, 2, 3, 4, 5]`.\n",
        "\n",
        "Do this for both the Euclidean distance and the Manhattan distance for each value of `k`.\n",
        "\n",
        "Also run your evaluation function for each value of `k` (for both distance metrics) and print out the classification accuracy and F1 score.\n",
        "\n",
        "(5 marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G7b_CwyoB9gy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1 for Euclidean distance\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mk=1 for Euclidean distance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m correct_labels \u001b[39m=\u001b[39m label_train\n\u001b[0;32m---> 12\u001b[0m pred, gold \u001b[39m=\u001b[39m k_fold_cross_validation(\u001b[39m5\u001b[39;49m, \u001b[39m1\u001b[39;49m, data_train, euclidean_distance)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Initialize variables to accumulate results\u001b[39;00m\n\u001b[1;32m     15\u001b[0m all_confusion_matrices \u001b[39m=\u001b[39m []\n",
            "Cell \u001b[0;32mIn[25], line 27\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(num_folds, k, dataset, distanceFunction)\u001b[0m\n\u001b[1;32m     25\u001b[0m y_test \u001b[39m=\u001b[39m test[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m classifier\u001b[39m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m---> 27\u001b[0m predict_label \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(x_test, distanceFunction)\n\u001b[1;32m     28\u001b[0m temp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(predict_label)\n\u001b[1;32m     29\u001b[0m prediction_list\u001b[39m.\u001b[39mappend(temp)\n",
            "Cell \u001b[0;32mIn[13], line 78\u001b[0m, in \u001b[0;36mkNearestNeighbours.predict\u001b[0;34m(self, X_test, distanceFunction)\u001b[0m\n\u001b[1;32m     74\u001b[0m ans_arr \u001b[39m=\u001b[39m []\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(X_test)):\n\u001b[1;32m     77\u001b[0m \t\u001b[39m#getting k narest neighbours for every point in the test data\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \tindices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_neighbors(X_test[i], distanceFunction)\n\u001b[1;32m     79\u001b[0m \tflag \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \tk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_neighbors\n",
            "Cell \u001b[0;32mIn[13], line 45\u001b[0m, in \u001b[0;36mkNearestNeighbours.get_neighbors\u001b[0;34m(self, x, distanceFunction)\u001b[0m\n\u001b[1;32m     39\u001b[0m \ttemp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX[i, :]\n\u001b[1;32m     41\u001b[0m \t\u001b[39m# #if the same point used\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \t\u001b[39m# if np.array_equal(x, temp):\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \t\u001b[39m# \tcontinue\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \td \u001b[39m=\u001b[39m distanceFunction(x, temp)\n\u001b[1;32m     46\u001b[0m \tans\u001b[39m.\u001b[39mappend((d, i))\n\u001b[1;32m     48\u001b[0m ans\u001b[39m.\u001b[39msort()\n",
            "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36meuclidean_distance\u001b[0;34m(vector1, vector2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meuclidean_distance\u001b[39m(vector1, vector2):\n\u001b[1;32m      2\u001b[0m     \u001b[39m# TODO:Implement the euclidean distance function (2 marks)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     t \u001b[39m=\u001b[39m vector1 \u001b[39m-\u001b[39m vector2\n\u001b[0;32m----> 5\u001b[0m     ans \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msqrt(np\u001b[39m.\u001b[39;49mdot(t\u001b[39m.\u001b[39;49mT, t))\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m ans\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# TODO: Perform cross-validation and then run your evaluation function for k=1, printing the accuracy and macro-average F1 score.\n",
        "\n",
        "accuracy_list_euclidean = []\n",
        "macro_list_euclidean = []\n",
        "\n",
        "accuracy_list_manhattan = []\n",
        "macro_list_manhattan = []\n",
        "\n",
        "print(\"k=1 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 1, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=1 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 1, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)\n",
        "## Code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=2\n",
            "k=2 for Euclidean distance\n",
            "Fold 1:\n",
            "Accuracy : 94.87394957983193\n",
            "MacroF1 : 0.9484826007205491\n",
            "Fold 2:\n",
            "Accuracy : 95.12605042016806\n",
            "MacroF1 : 0.9511954325223366\n",
            "Fold 3:\n",
            "Accuracy : 94.78991596638654\n",
            "MacroF1 : 0.9473600557983854\n",
            "Fold 4:\n",
            "Accuracy : 95.16806722689076\n",
            "MacroF1 : 0.9510515270551221\n",
            "Fold 5:\n",
            "Accuracy : 95.16806722689076\n",
            "MacroF1 : 0.9511924757828195\n",
            "\n",
            "Overall Metrics:\n",
            "Average Accuracy : 95.0252100840336\n",
            "Average MacroF1 : 0.9498564183758426\n"
          ]
        }
      ],
      "source": [
        "print(\"k=2 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 2, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=2 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 2, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"k=3 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 3, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=3 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 3, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"k=4 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 4, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=4 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 4, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"k=5 for Euclidean distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 5, data_train, euclidean_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_euclidean.append(average_accuracy)\n",
        "macro_list_euclidean.append(average_macroF1)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"k=5 for Manhattan distance\")\n",
        "correct_labels = label_train\n",
        "\n",
        "pred, gold = k_fold_cross_validation(5, 5, data_train, manhattan_distance)\n",
        "\n",
        "# Initialize variables to accumulate results\n",
        "all_confusion_matrices = []\n",
        "all_accuracies = []\n",
        "all_macroF1s = []\n",
        "\n",
        "for i in range(0, len(pred)):\n",
        "\n",
        "    print(f\"Fold {i + 1}:\")\n",
        "    predictions = pred[i]\n",
        "    \n",
        "    confusion_matrix, accuracy, macroF1 = evaluation(predictions, gold[i])\n",
        "    all_confusion_matrices.append(confusion_matrix)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_macroF1s.append(macroF1)\n",
        "    print(f\"Accuracy : {accuracy}\\nMacroF1 : {macroF1}\")\n",
        "\n",
        "# Calculate overall metrics (e.g., average accuracy and macroF1)\n",
        "average_accuracy = sum(all_accuracies) / len(all_accuracies)\n",
        "average_macroF1 = sum(all_macroF1s) / len(all_macroF1s)\n",
        "\n",
        "# Optionally, you can also print the overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Average Accuracy : {average_accuracy}\")\n",
        "print(f\"Average MacroF1 : {average_macroF1}\")\n",
        "\n",
        "accuracy_list_manhattan.append(average_accuracy)\n",
        "macro_list_manhattan.append(average_macroF1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH6EG8d0R4s"
      },
      "source": [
        "Next, present the results as a graph with `k` values on the x-axis and classification accuracy on the y-axis.\n",
        "\n",
        "Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric). Make another graph but with the F1-score on the y-axis this time. The graphs should be properly labeled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcUclERi0fmg",
        "outputId": "d6a70e55-f0ba-4844-d227-b3bd7c5cc5cc"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and classification accuracy on the y-axis\n",
        "\n",
        "k_values = np.arange(1, 6)  # k values from 1 to 5\n",
        "\n",
        "# Create a single plot to compare the two versions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, accuracy_list_euclidean, marker='o', label='Euclidean Distance')\n",
        "plt.plot(k_values, accuracy_list_manhattan, marker='o', label='Manhattan Distance')\n",
        "\n",
        "plt.title('Classification Accuracy vs. k Value')\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('Classification Accuracy')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVZA1m1w6Sh",
        "outputId": "223c1b1b-6078-446c-d788-8646f79c4b4e"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis\n",
        "\n",
        "k_values = np.arange(1, 6)  # k values from 1 to 5\n",
        "\n",
        "# Create a single plot to compare the two versions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, macro_list_euclidean, marker='o', label='Euclidean Distance')\n",
        "plt.plot(k_values, macro_list_manhattan, marker='o', label='Manhattan Distance')\n",
        "\n",
        "plt.title('Classification F1 vs. k Value')\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('F1 Accuracy')\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9MjHsLCx1Dic"
      },
      "source": [
        "Comment on the best value of k you have found for both distance metrics using\n",
        "cross-validation. What impact does this value have on the decision boundries generated by the model and the stability of decisions?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JDqvNjX1KUt"
      },
      "outputs": [],
      "source": [
        "## (2 marks)\n",
        "# TODO: Write your answer below as a comment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eulkPB4y1dgM"
      },
      "source": [
        "Finally, use the best value of `k` for both distance metrics and run it on the test dataset.\n",
        "\n",
        "Find the confusion matrix, classification accuracy and F1 score and print them.\n",
        "\n",
        "The confusion matrix must be displayed as a heatmap annotated with the data values. The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU40KNeb80Xa"
      },
      "outputs": [],
      "source": [
        "## (3 marks)\n",
        "# TODO: Use the best value of k on test dataset (for both distance metrics).\n",
        "\n",
        "\n",
        "# Code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GZwA3ffh9ZfK"
      },
      "source": [
        "## **Part 2: Implement using Scikit-Learn (25 marks)**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SB6Uqhlt9n7P"
      },
      "source": [
        "In this part, you have to use [scikit-learn's k-NN implementation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) to train and test your classifier on the dataset used in Part 1. Repeat the tasks you have done in Part 1 but this time using scikit-learn.\n",
        "\n",
        "- Perform 5-fold cross-validation and run the k-NN classifier for values of `k = [1, 2, 3, 4, 5]` using both Euclidean and Manhattan distance.\n",
        "\n",
        "- Use scikit-learn's [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function to calculate the accuracy, the [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to calculate macro-average F1 score,\n",
        "  and the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function to calculate confusion matrix from the predicted labels.\n",
        "\n",
        "- Present the results as a graph with k values on the x-axis and performance measures on the y-axis just like you did in Part 1. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0_GlUmZw6Si"
      },
      "outputs": [],
      "source": [
        "# (10 marks)\n",
        "# TODO:  Perform 5-fold cross-validation.\n",
        "# Code here\n",
        "\n",
        "classifier = KNeighborsClassifier(n_neighbors=3) \n",
        "\n",
        "scores = cross_val_score(classifier, X, y, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "jydXs3tyw6Si",
        "outputId": "b21e0684-f4ab-4ecd-c342-8d0fca9df142"
      },
      "outputs": [],
      "source": [
        "# (5 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and classifcation accuracy on the y-axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, print the best values of k for both distance metrics. Then use these values of k on the test dataset and print the evaluation scores and confusion matrix (as a heatmap) for each of the distance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "# TODO: Use the best value of k on test dataset (for both distance metrics).\n",
        "\n",
        "# Code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "baKb0CRZ7uqU"
      },
      "source": [
        "# Testing the classifier with your own handwriting!\n",
        "\n",
        "Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications.\n",
        "\n",
        "We can use the sketchpad interface to write our digits and pass that to our classifier. Try it out below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MuDkj5rr7tQ-"
      },
      "outputs": [],
      "source": [
        "# !pip install gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "52hF4s26UG3p",
        "outputId": "ced8e2d5-4570-4492-d25d-729494b98579"
      },
      "outputs": [],
      "source": [
        "#### Initialize your classifier here ####\n",
        "\n",
        "#### Try this with out with your own classifiers\n",
        "\n",
        "my_classifier = kNearestNeighbours(10)\n",
        "my_classifier.fit(input_train, label_train)\n",
        "\n",
        "# my_sk_classifer = KNeighborsClassifier(n_neighbors=???,metric=???)\n",
        "# my_sk_classifer.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TSbEjbZ57tRN"
      },
      "outputs": [],
      "source": [
        "def sketch_recognition(img):\n",
        "    image_arr = img.flatten()\n",
        "\n",
        "    label = my_classifier.predict([image_arr],euclidean_distance)\n",
        "\n",
        "    # label = my_sk_classifer.predict([image_arr])\n",
        "\n",
        "    print(\"Your label: \", label[0])\n",
        "    return str(label[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "wn2EE8ONUWG7",
        "outputId": "fa023eb7-8f9c-4daf-8862-053e87d27f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your label:  7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shaheer/.local/lib/python3.8/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your label:  4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shaheer/.local/lib/python3.8/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your label:  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shaheer/.local/lib/python3.8/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your label:  2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shaheer/.local/lib/python3.8/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iface = gr.Interface(fn=sketch_recognition, inputs=\"sketchpad\", outputs=\"text\")\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "TZ5EqZS0Sd7g",
        "outputId": "9cf85166-9d55-4da1-de2a-056866e2be5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shaheer/.local/lib/python3.8/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "### Draw and save your handwritten digits as an image (if you want)\n",
        "\n",
        "gr.Interface(lambda x: x, \"sketchpad\", \"image\").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## (5 marks)\n",
        "## TODO: Use images of your own handwritten digits and make a prediction using your own classifier.\n",
        "## You can use the sketchpad above if you'd like or even use pencil and paper to write out some numbers and then take a picture.\n",
        "\n",
        "\n",
        "## Display five of these images and print out the corresponding prediction\n",
        "## You can use either your own classifier or sklearn.\n",
        "\n",
        "## Have fun!!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fin.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
